{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
    "\n",
    "file_name = \"horse-or-human.zip\"\n",
    "training_dir = \"horse-or-human/training\"\n",
    "urllib.request.urlretrieve(url, file_name)\n",
    "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "zip_ref.extractall(training_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "train_generator = train_datagen.flow_from_directory(training_dir, target_size=(300,300), class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, BatchNormalization, Activation, MaxPool2D, GlobalAveragePooling2D, Add, Dropout\n",
    "from tensorflow.keras import Model\n",
    "## resnet paper - https://arxiv.org/pdf/1512.03385.pdf\n",
    "## issue may be images are too small to run convolutions,\n",
    "## but residual blocks should provide some accuracy bumps\n",
    "\n",
    "class ResidualBlock(Model):\n",
    "    def __init__(self, channel_in = 64, channel_out = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        channel = channel_out // 4\n",
    "        self.dropout = Dropout(.2)\n",
    "        self.conv1 = Conv2D(channel, kernel_size = (1, 1), padding = \"same\")\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.av1 = Activation(tf.nn.relu)\n",
    "        self.conv2 = Conv2D(channel, kernel_size = (3, 3), padding = \"same\")\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.av2 = Activation(tf.nn.relu)\n",
    "        self.conv3 = Conv2D(channel_out, kernel_size = (1, 1), padding = \"same\")\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.shortcut = self._shortcut(channel_in, channel_out)\n",
    "        self.add = Add()\n",
    "        self.av3 = Activation(tf.nn.relu)\n",
    "        \n",
    "    def call(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.av1(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.av2(h)\n",
    "        h = self.conv3(h)\n",
    "        h = self.bn3(h)\n",
    "        shortcut = self.shortcut(x)\n",
    "        h = self.add([h, shortcut])\n",
    "        y = self.av3(h)\n",
    "        return y\n",
    "    \n",
    "    def _shortcut(self, channel_in, channel_out):\n",
    "        if channel_in == channel_out:\n",
    "            return lambda x : x\n",
    "        else:\n",
    "            return self._projection(channel_out)\n",
    "        \n",
    "    def _projection(self, channel_out):\n",
    "        return Conv2D(channel_out, kernel_size = (1, 1), padding = \"same\")\n",
    "           \n",
    "class LightNet18(Model):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        super().__init__()                \n",
    "        self._layers = [\n",
    "            # conv1\n",
    "            Conv2D(64, input_shape = input_shape, kernel_size = (7, 7), strides=(2, 2), padding = \"same\"),\n",
    "            BatchNormalization(),\n",
    "            Activation(tf.nn.relu),\n",
    "            # conv2_x\n",
    "            MaxPool2D(pool_size = (3, 3), strides = (2, 2), padding = \"same\"),\n",
    "            ResidualBlock(64, 64),\n",
    "            # conv3_x\n",
    "            Conv2D(128, kernel_size = (1, 1), strides=(2, 2)),\n",
    "            ResidualBlock(128, 128),\n",
    "            # conv4_x\n",
    "            Conv2D(256, kernel_size = (1, 1), strides=(2, 2)),\n",
    "            ResidualBlock(256, 256),\n",
    "            # conv5_x\n",
    "            Conv2D(512, kernel_size = (1, 1), strides=(2, 2)),\n",
    "            ResidualBlock(512, 512),\n",
    "            # last part\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(1000, activation = tf.nn.relu),\n",
    "            Dense(output_dim, activation = tf.nn.softmax)\n",
    "        ]\n",
    "        \n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            if isinstance(layer, list):\n",
    "                for l in layer:\n",
    "                    x = l(x)    \n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "       \n",
    "    \n",
    "model = tf.keras.applications.mobilenet_v2.MobileNetV2(weights=None,classes=2)\n",
    "model.build(input_shape=(300,300,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 4/33 [==>...........................] - ETA: 4:12 - loss: 0.8490 - accuracy: 0.5078"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(train_generator, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db633a2415b8a28dc946d9e13a3dbc9dc65443e0a408813de63e49810352fa2a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
